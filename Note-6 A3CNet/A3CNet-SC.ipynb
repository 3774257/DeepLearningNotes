{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sonnet.python.modules.basic import Linear\n",
    "from sonnet.python.modules.base import AbstractModule\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "_EPSILON = 1e-6 # avoid nan\n",
    "ENTROPY_BETA = 1\n",
    "GAMMA = 0.9 # discount factor\n",
    "VALUE_BETA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swich(tensor):\n",
    "    return tensor * tf.nn.sigmoid(tensor + _EPSILON)\n",
    "\n",
    "# shared neural network\n",
    "def _build_shared_network(inputs):\n",
    "    # inputs [batch_size, state_size]\n",
    "    network = Linear(32, 'input_layer')(inputs)\n",
    "    return swich(network)\n",
    "\n",
    "\n",
    "class build_policy_network(AbstractModule):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def _build(self, inputs, action_size):\n",
    "        shared_network = _build_shared_network(inputs)\n",
    "        policy = Linear(32, 'policy_input')(shared_network)\n",
    "        policy = swich(policy)\n",
    "        policy = Linear(action_size, 'policy_output')(policy)\n",
    "        return tf.nn.softmax(policy + _EPSILON) # avoid nan   \n",
    "\n",
    "    \n",
    "class build_value_network(AbstractModule):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def _build(self, inputs):\n",
    "        shared_network = _build_shared_network(inputs)\n",
    "        value = Linear(32, 'value_input')(shared_network)\n",
    "        value = swich(value)\n",
    "        value = Linear(1, 'value_output')(value)\n",
    "        return value\n",
    "    \n",
    "    \n",
    "# build approximate neural network\n",
    "def _build_approximate_network(inputs, action_size):\n",
    "    shared_network = _build_shared_network(inputs)\n",
    "    \n",
    "    policy = Linear(32, 'policy_input')(shared_network)\n",
    "    policy = swich(policy)\n",
    "    policy = Linear(action_size, 'policy_output')(policy)\n",
    "    policy = tf.nn.softmax(policy + _EPSILON) # avoid nan   \n",
    "    \n",
    "    value = Linear(32, 'value_input')(shared_network)\n",
    "    value = swich(value)\n",
    "    value = Linear(1, 'value_output')(value)\n",
    "    return policy, value\n",
    "\n",
    "class simple_approximate_network(AbstractModule):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def _build(self, inputs, action_size):\n",
    "        return _build_approximate_network(inputs, action_size)\n",
    "        \n",
    "# batch gather function from https://github.com/deepmind/dnc/blob/master/util.py\n",
    "def _batch_gather(values, indices):\n",
    "    \"\"\"Returns batched `tf.gather` for every row in the input.\"\"\"\n",
    "    with tf.name_scope('batch_gather', values=[values, indices]):\n",
    "        unpacked = zip(tf.unstack(values), tf.unstack(indices))\n",
    "        result = [tf.gather(value, index) for value, index in unpacked]\n",
    "        return tf.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global network for buffer weights and calculate gardients\n",
    "class Access(object):\n",
    "    def __init__(self, state_size, action_size, name='access'):\n",
    "        #variable_scope for more clear graph, not necessary\n",
    "        with tf.variable_scope(name):                   \n",
    "            # placeholder for state and next state or you may like call it observation\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, state_size], 'inputs')     \n",
    "            #self.network = simple_approximate_network('global_network')\n",
    "            #self.policy, self.value = self.network(self.inputs, action_size)\n",
    "            self.policy_network = build_policy_network('global_policy')\n",
    "            self.value_network = build_value_network('global_value')\n",
    "            self.policy = self.policy_network(self.inputs, action_size)\n",
    "            self.value = self.value_network(self.inputs)\n",
    "            \n",
    "        self.optimizer_actor = tf.train.RMSPropOptimizer(LEARNING_RATE, name='optimizer_actor')\n",
    "        self.optimizer_critic = tf.train.RMSPropOptimizer(LEARNING_RATE, name='optimizer_critic') \n",
    "        \n",
    "    def get_trainable_variables(self):\n",
    "        return [self.policy_network.get_variables(), self.value_network.get_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# local network for advantage actor-critic which are also know as A2C\n",
    "class ACNet(object):\n",
    "    def __init__(self, Access, state_size, action_size, name):\n",
    "        self.Access = Access\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # action space, we assume that action space is range(0 to action_size-1)\n",
    "        self.action_space = np.arange(action_size, dtype=np.int32)\n",
    "        \n",
    "        #variable_scope local graph, necessary\n",
    "        with tf.variable_scope(name):\n",
    "            # placeholder for state and next state or you may like call it observation\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, state_size], 'inputs')   \n",
    "            self.action = tf.placeholder(tf.int32, [None], 'action')\n",
    "            # n-step reward and discounted n next step value\n",
    "            self.target = tf.placeholder(tf.float32, [None, 1], 'target')\n",
    "            \n",
    "            self.policy_network = build_policy_network('global_policy')\n",
    "            self.value_network = build_value_network('global_value')\n",
    "            self.policy = self.policy_network(self.inputs, action_size)\n",
    "            self.value = self.value_network(self.inputs)\n",
    "            \n",
    "            self._build_loss_function()\n",
    "            self.update_local, self.update_access = self._build_update()         \n",
    "        \n",
    "    def _build_loss_function(self):\n",
    "        self.advantage = self.target - self.value\n",
    "        # value loss\n",
    "        self.value_loss = tf.reduce_mean(tf.square(self.advantage))\n",
    "    \n",
    "        # policy loss\n",
    "        # get the stochastic policy action probability\n",
    "        #policy_action = _batch_gather(self.policy, self.action)\n",
    "        action_onehot = tf.one_hot(self.action, self.action_size)\n",
    "        policy_action = tf.reduce_sum(self.policy * action_onehot, axis=1, keep_dims=True)\n",
    "        log_policy_action = tf.log(policy_action + _EPSILON)\n",
    "        # no grad pass through advantage in actor network \n",
    "        policy_loss = tf.stop_gradient(self.advantage) * tf.expand_dims(log_policy_action, axis=1)\n",
    "        # entropy loss\n",
    "        entropy_loss = tf.reduce_mean(self.policy * tf.log(self.policy + _EPSILON), axis=1, keep_dims=True)\n",
    "        self.policy_loss = tf.reduce_mean(policy_loss + ENTROPY_BETA * entropy_loss)\n",
    "\n",
    "        # adjust some params\n",
    "        self.a_policy_loss = tf.reduce_mean(policy_loss)\n",
    "        self.a_entropy_loss = tf.reduce_mean(ENTROPY_BETA * entropy_loss)\n",
    "        self.a_value_loss = self.value_loss\n",
    "\n",
    "    def _build_update(self):\n",
    "        global_policy_params, global_value_params = self.Access.get_trainable_variables()\n",
    "        local_policy_params = self.policy_network.get_variables()\n",
    "        local_value_params = self.value_network.get_variables()\n",
    "        \n",
    "        policy_list = []\n",
    "        for g,l in zip(global_policy_params, local_policy_params):\n",
    "            policy_list.append(l.assign(g))\n",
    "            \n",
    "        value_list = []\n",
    "        for g,l in zip(global_value_params, local_value_params):\n",
    "            value_list.append(l.assign(g))        \n",
    "        \n",
    "        policy_grad = tf.gradients(self.policy_loss, list(local_policy_params))\n",
    "        value_grad = tf.gradients(self.value_loss, list(local_value_params))\n",
    "        \n",
    "        policy_apply = self.Access.optimizer_actor.apply_gradients(zip(policy_grad, list(global_policy_params)))\n",
    "        value_apply = self.Access.optimizer_critic.apply_gradients(zip(value_grad, list(global_value_params)))\n",
    "        return [policy_list, value_list], [policy_apply, value_apply]    \n",
    "    \n",
    "    def choose_action(self, SESS, state):  # run by a local\n",
    "        policy = SESS.run(self.policy, {self.inputs: np.expand_dims(state, axis=0)})\n",
    "        policy = np.squeeze(policy)\n",
    "        action = np.random.choice(self.action_space, 1, p=policy)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    def __init__(self, master, name, state_size, action_size):\n",
    "        self.env = gym.make(GAME).unwrapped\n",
    "        self.master = master\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.worker = ACNet(self.master, self.state_size, self.action_size, name)\n",
    "    \n",
    "    def work(self, SESS):\n",
    "        worker = self.worker\n",
    "        env = self.env\n",
    "        \n",
    "        episode_score_list = []\n",
    "        episode = 0\n",
    "        while episode < MAX_EPISODES:\n",
    "\n",
    "            t_start = t = 1\n",
    "            state = env.reset()\n",
    "\n",
    "            buffer_state = []\n",
    "            buffer_reward = []\n",
    "            buffer_next_state = []\n",
    "            episode_score = 0\n",
    "        \n",
    "            while True:\n",
    "                SESS.run(worker.update_local)\n",
    "                action = worker.choose_action(SESS, state)[0]\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                episode_score += reward\n",
    "\n",
    "                buffer_state.append(state)\n",
    "                buffer_reward.append(reward)\n",
    "                buffer_next_state.append(next_state)\n",
    "                state = next_state\n",
    "\n",
    "                if t - t_start == T_MAX or done:\n",
    "                    t_start = t\n",
    "\n",
    "                    if done:\n",
    "                        state_value = 0\n",
    "                    else:\n",
    "                        state_value = SESS.run(worker.value, {worker.inputs:np.expand_dims(state, axis=0)})[0][0]\n",
    "\n",
    "                    buffer_target = []\n",
    "                    for r in buffer_reward[:-1][::-1]:\n",
    "                        state_value = r + GAMMA * state_value\n",
    "                        buffer_target.append(state_value)\n",
    "                    buffer_target.reverse()\n",
    "\n",
    "                    feed_dict = {worker.inputs: np.vstack(buffer_state[:-1]), \n",
    "                                 worker.action: np.squeeze(np.vstack(buffer_reward[:-1]), axis=1), \n",
    "                                 worker.target: np.expand_dims(np.array(buffer_target), axis=1)}\n",
    "                    SESS.run(worker.update_access, feed_dict)\n",
    "\n",
    "                    if done:\n",
    "                        if episode >= 99:\n",
    "                            entropy_loss, policy_loss, value_loss = SESS.run(\n",
    "                                [worker.a_entropy_loss, worker.a_policy_loss, worker.a_value_loss],\n",
    "                                feed_dict)         \n",
    "                            print (entropy_loss, policy_loss, value_loss)\n",
    "\n",
    "                            #policy, advantage = SESS.run([worker.policy, worker.advantage], feed_dict)\n",
    "                            #print (policy)\n",
    "                            #print (advantage)\n",
    "\n",
    "                    buffer_state = [buffer_state[-1]]\n",
    "                    buffer_reward = [buffer_reward[-1]]\n",
    "                    buffer_next_state = [buffer_next_state[-1]]\n",
    "\n",
    "                t += 1\n",
    "                if done:\n",
    "                    episode +=1\n",
    "                    episode_score_list.append(episode_score)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.000137517 -0.701993 0.00761468\n",
      "-0.000153595 7.18923 0.336633\n",
      "-0.000161138 -1.17689 0.0170028\n",
      "-0.000425137 7.50895 0.376134\n",
      "-0.000167263 -2.47374 0.0452031\n",
      "-9.19674e-05 -1.22811 0.0166318\n",
      "-0.000175377 4.39671 0.137429\n",
      "-0.000189629 6.76128 0.298914\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import multiprocessing\n",
    "import threading\n",
    "NUMS_CPU = multiprocessing.cpu_count()\n",
    "\n",
    "GAME = 'CartPole-v0'\n",
    "env = gym.make(GAME)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "MAX_EPISODES = 100\n",
    "T_MAX = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "SESS = tf.Session()\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    master = Access(state_size, action_size)\n",
    "    worker_list = []\n",
    "    for i in range(NUMS_CPU):\n",
    "        worker_list.append(Worker(master, 'W%i'%i, state_size, action_size))\n",
    "        \n",
    "    COORD = tf.train.Coordinator()\n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "    \n",
    "    worker_threads = []\n",
    "    for worker in worker_list:\n",
    "        job = lambda: worker.work(SESS)\n",
    "        t = threading.Thread(target=job)\n",
    "        t.start()\n",
    "        worker_threads.append(t)\n",
    "    COORD.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
