{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sonnet.python.modules.basic import Linear\n",
    "from sonnet.python.modules.base import AbstractModule\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "_EPSILON = 1e-6 # avoid nan\n",
    "ENTROPY_BETA = 1\n",
    "GAMMA = 0.9 # discount factor\n",
    "VALUE_BETA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swich(tensor):\n",
    "    return tensor * tf.nn.sigmoid(tensor + _EPSILON)\n",
    "\n",
    "# shared neural network\n",
    "def _build_shared_network(inputs):\n",
    "    # inputs [batch_size, state_size]\n",
    "    network = Linear(32, 'input_layer')(inputs)\n",
    "    return swich(network)\n",
    "\n",
    "# build approximate neural network\n",
    "def _build_approximate_network(inputs, action_size):\n",
    "    shared_network = _build_shared_network(inputs)\n",
    "    \n",
    "    policy = Linear(32, 'policy_input')(shared_network)\n",
    "    policy = swich(policy)\n",
    "    policy = Linear(action_size, 'policy_output')(policy)\n",
    "    policy = tf.nn.softmax(policy + _EPSILON) # avoid nan   \n",
    "    \n",
    "    value = Linear(32, 'value_input')(shared_network)\n",
    "    value = swich(value)\n",
    "    value = Linear(1, 'value_output')(value)\n",
    "    return policy, value\n",
    "\n",
    "class simple_approximate_network(AbstractModule):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def _build(self, inputs, action_size):\n",
    "        return _build_approximate_network(inputs, action_size)\n",
    "        \n",
    "# batch gather function from https://github.com/deepmind/dnc/blob/master/util.py\n",
    "def _batch_gather(values, indices):\n",
    "    \"\"\"Returns batched `tf.gather` for every row in the input.\"\"\"\n",
    "    with tf.name_scope('batch_gather', values=[values, indices]):\n",
    "        unpacked = zip(tf.unstack(values), tf.unstack(indices))\n",
    "        result = [tf.gather(value, index) for value, index in unpacked]\n",
    "        return tf.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global network for buffer weights and calculate gardients\n",
    "class Access(object):\n",
    "    def __init__(self, state_size, action_size, name='access'):\n",
    "        #variable_scope for more clear graph, not necessary\n",
    "        with tf.variable_scope(name):                   \n",
    "            # placeholder for state and next state or you may like call it observation\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, state_size], 'inputs')     \n",
    "            self.network = simple_approximate_network('global_network')\n",
    "            self.policy, self.value = self.network(self.inputs, action_size)\n",
    "            \n",
    "        self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "        #self.optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, 0.99, name='optimizer')\n",
    "        \n",
    "    def get_trainable_variables(self):\n",
    "        return self.network.get_variables()\n",
    "    \n",
    "    \n",
    "# local network for advantage actor-critic which are also know as A2C\n",
    "class ACNet(object):\n",
    "    def __init__(self, Access, state_size, action_size, name):\n",
    "        self.Access = Access\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # action space, we assume that action space is range(0 to action_size-1)\n",
    "        self.action_space = np.arange(action_size, dtype=np.int32)\n",
    "        \n",
    "        #variable_scope local graph, necessary\n",
    "        with tf.variable_scope(name):\n",
    "            # placeholder for state and next state or you may like call it observation\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, state_size], 'inputs')   \n",
    "            self.action = tf.placeholder(tf.int32, [None], 'action')\n",
    "            # n-step reward and discounted n next step value\n",
    "            self.target = tf.placeholder(tf.float32, [None, 1], 'target')\n",
    "            \n",
    "            self.network = simple_approximate_network('ACNet')\n",
    "            self.policy, self.value = self.network(self.inputs, action_size)\n",
    "            \n",
    "            self._build_loss_function()\n",
    "            self.update_local, self.update_access = self._build_update()         \n",
    "        \n",
    "    def _build_loss_function(self):\n",
    "        self.advantage = self.target - self.value\n",
    "        # value loss\n",
    "        self.value_loss = tf.reduce_mean(tf.square(self.advantage))\n",
    "    \n",
    "        # policy loss\n",
    "        # get the stochastic policy action probability\n",
    "        #policy_action = _batch_gather(self.policy, self.action)\n",
    "        action_onehot = tf.one_hot(self.action, self.action_size)\n",
    "        policy_action = tf.reduce_sum(self.policy * action_onehot, axis=1, keep_dims=True)\n",
    "        print (action_onehot)\n",
    "        print (policy_action)\n",
    "\n",
    "        log_policy_action = tf.log(policy_action + _EPSILON)\n",
    "        # no grad pass through advantage in actor network \n",
    "        policy_loss = tf.stop_gradient(self.advantage*0.1) * tf.expand_dims(log_policy_action, axis=1)\n",
    "        # entropy loss\n",
    "        entropy_loss = tf.reduce_mean(self.policy * tf.log(self.policy + _EPSILON), axis=1, keep_dims=True)\n",
    "        self.policy_loss = tf.reduce_mean(policy_loss + ENTROPY_BETA * entropy_loss)\n",
    "        \n",
    "        self.total_loss = VALUE_BETA * self.value_loss + self.policy_loss\n",
    "        # adjust some params\n",
    "        self.a_policy_loss = tf.reduce_mean(policy_loss)\n",
    "        self.a_entropy_loss = tf.reduce_mean(entropy_loss)\n",
    "        self.a_value_loss = self.value_loss\n",
    "\n",
    "    def _build_update(self):\n",
    "        global_params = list(self.Access.get_trainable_variables())\n",
    "        local_params = list(self.get_trainable_variables())\n",
    "        \n",
    "        # update local network weights\n",
    "        zip_list = []\n",
    "        for g,l in zip(global_params, local_params):\n",
    "            zip_list.append(l.assign(g))\n",
    "        \n",
    "        # update global network gradients\n",
    "        local_grads = tf.gradients(self.total_loss, local_params)\n",
    "        apply_gradients = self.Access.optimizer.apply_gradients(zip(local_grads, global_params))\n",
    "        return zip_list, apply_gradients    \n",
    "    \n",
    "    def get_trainable_variables(self):\n",
    "        return self.network.get_variables()\n",
    "    \n",
    "    def choose_action(self, SESS, state):  # run by a local\n",
    "        policy = SESS.run(self.policy, {self.inputs: np.expand_dims(state, axis=0)})\n",
    "        policy = np.squeeze(policy)\n",
    "        action = np.random.choice(self.action_space, 1, p=policy)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "GAME = 'CartPole-v0'\n",
    "env = gym.make(GAME)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "MAX_EPISODES = 100000\n",
    "T_MAX = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "SESS = tf.Session()\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    master = Access(state_size, action_size)\n",
    "    worker = ACNet(master, state_size, action_size, 'W0')\n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "    \n",
    "    episode_score_list = []\n",
    "    episode = 0\n",
    "    while episode < MAX_EPISODES:\n",
    "        if episode < 700:\n",
    "            ENTROPY_BETA = 10\n",
    "        else:\n",
    "            ENTROPY_BETA = 1\n",
    "        \n",
    "        t_start = t = 1\n",
    "        state = env.reset()\n",
    "        \n",
    "        buffer_state = []\n",
    "        buffer_reward = []\n",
    "        buffer_next_state = []\n",
    "        episode_score = 0\n",
    "        \n",
    "        while True:\n",
    "            SESS.run(worker.update_local)\n",
    "            action = worker.choose_action(SESS, state)[0]\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_score += reward\n",
    "            \n",
    "            buffer_state.append(state)\n",
    "            buffer_reward.append(reward)\n",
    "            buffer_next_state.append(next_state)\n",
    "            state = next_state\n",
    "            \n",
    "            \n",
    "            if t - t_start == T_MAX or done:\n",
    "                t_start = t\n",
    "                            \n",
    "                if done:\n",
    "                    state_value = 0\n",
    "                else:\n",
    "                    state_value = SESS.run(worker.value, {worker.inputs:np.expand_dims(state, axis=0)})[0][0]\n",
    "                    \n",
    "                buffer_target = []\n",
    "                for r in buffer_reward[:-1][::-1]:\n",
    "                    state_value = r + GAMMA * state_value\n",
    "                    buffer_target.append(state_value)\n",
    "                buffer_target.reverse()\n",
    "                \n",
    "                feed_dict = {worker.inputs: np.vstack(buffer_state[:-1]), \n",
    "                             worker.action: np.squeeze(np.vstack(buffer_reward[:-1]), axis=1), \n",
    "                             worker.target: np.expand_dims(np.array(buffer_target), axis=1)}\n",
    "                SESS.run(worker.update_access, feed_dict)\n",
    "                \n",
    "                if done:\n",
    "                    if episode > 99990:\n",
    "                        entropy_loss, policy_loss, value_loss, total_loss = SESS.run(\n",
    "                            [worker.a_entropy_loss, worker.a_policy_loss, worker.a_value_loss, worker.total_loss],\n",
    "                            feed_dict)         \n",
    "                        print (entropy_loss, policy_loss, value_loss, total_loss)\n",
    "\n",
    "                        policy, advantage = SESS.run([worker.policy, worker.advantage], feed_dict)\n",
    "                        print (policy)\n",
    "                        print (advantage)\n",
    "                \n",
    "                    \n",
    "                \n",
    "                buffer_state = [buffer_state[-1]]\n",
    "                buffer_reward = [buffer_reward[-1]]\n",
    "                buffer_next_state = [buffer_next_state[-1]]\n",
    "                \n",
    "            t += 1\n",
    "            if done:\n",
    "                episode +=1\n",
    "                episode_score_list.append(episode_score)\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "pd.Series(episode_score_list).plot(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_score_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
