{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sonnet.python.modules.basic import Linear\n",
    "from sonnet.python.modules.base import AbstractModule\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "_EPSILON = 1e-6 # avoid nan\n",
    "ENTROPY_BETA = 1\n",
    "GAMMA = 0.9 # discount factor\n",
    "VALUE_BETA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swich(tensor):\n",
    "    return tensor * tf.nn.sigmoid(tensor + _EPSILON)\n",
    "\n",
    "# shared neural network\n",
    "def _build_shared_network(inputs):\n",
    "    # inputs [batch_size, state_size]\n",
    "    network = Linear(32, 'input_layer')(inputs)\n",
    "    return swich(network)\n",
    "\n",
    "\n",
    "class build_policy_network(AbstractModule):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def _build(self, inputs, action_size):\n",
    "        shared_network = _build_shared_network(inputs)\n",
    "        policy = Linear(32, 'policy_input')(shared_network)\n",
    "        policy = swich(policy)\n",
    "        policy = Linear(action_size, 'policy_output')(policy)\n",
    "        return tf.nn.softmax(policy + _EPSILON) # avoid nan   \n",
    "\n",
    "    \n",
    "class build_value_network(AbstractModule):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def _build(self, inputs):\n",
    "        shared_network = _build_shared_network(inputs)\n",
    "        value = Linear(32, 'value_input')(shared_network)\n",
    "        value = swich(value)\n",
    "        value = Linear(1, 'value_output')(value)\n",
    "        return value\n",
    "    \n",
    "    \n",
    "# build approximate neural network\n",
    "def _build_approximate_network(inputs, action_size):\n",
    "    shared_network = _build_shared_network(inputs)\n",
    "    \n",
    "    policy = Linear(32, 'policy_input')(shared_network)\n",
    "    policy = swich(policy)\n",
    "    policy = Linear(action_size, 'policy_output')(policy)\n",
    "    policy = tf.nn.softmax(policy + _EPSILON) # avoid nan   \n",
    "    \n",
    "    value = Linear(32, 'value_input')(shared_network)\n",
    "    value = swich(value)\n",
    "    value = Linear(1, 'value_output')(value)\n",
    "    return policy, value\n",
    "\n",
    "class simple_approximate_network(AbstractModule):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def _build(self, inputs, action_size):\n",
    "        return _build_approximate_network(inputs, action_size)\n",
    "        \n",
    "# batch gather function from https://github.com/deepmind/dnc/blob/master/util.py\n",
    "def _batch_gather(values, indices):\n",
    "    \"\"\"Returns batched `tf.gather` for every row in the input.\"\"\"\n",
    "    with tf.name_scope('batch_gather', values=[values, indices]):\n",
    "        unpacked = zip(tf.unstack(values), tf.unstack(indices))\n",
    "        result = [tf.gather(value, index) for value, index in unpacked]\n",
    "        return tf.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global network for buffer weights and calculate gardients\n",
    "class Access(object):\n",
    "    def __init__(self, state_size, action_size, name='access'):\n",
    "        #variable_scope for more clear graph, not necessary\n",
    "        with tf.variable_scope(name):                   \n",
    "            # placeholder for state and next state or you may like call it observation\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, state_size], 'inputs')     \n",
    "            #self.network = simple_approximate_network('global_network')\n",
    "            #self.policy, self.value = self.network(self.inputs, action_size)\n",
    "            self.policy_network = build_policy_network('global_policy')\n",
    "            self.value_network = build_value_network('global_value')\n",
    "            self.policy = self.policy_network(self.inputs, action_size)\n",
    "            self.value = self.value_network(self.inputs)\n",
    "            \n",
    "        self.optimizer_actor = tf.train.RMSPropOptimizer(LEARNING_RATE, name='optimizer_actor')\n",
    "        self.optimizer_critic = tf.train.RMSPropOptimizer(LEARNING_RATE, name='optimizer_critic') \n",
    "        \n",
    "    def get_trainable_variables(self):\n",
    "        return [self.policy_network.get_variables(), self.value_network.get_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# local network for advantage actor-critic which are also know as A2C\n",
    "class ACNet(object):\n",
    "    def __init__(self, Access, state_size, action_size, name):\n",
    "        self.Access = Access\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # action space, we assume that action space is range(0 to action_size-1)\n",
    "        self.action_space = np.arange(action_size, dtype=np.int32)\n",
    "        \n",
    "        #variable_scope local graph, necessary\n",
    "        with tf.variable_scope(name):\n",
    "            # placeholder for state and next state or you may like call it observation\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, state_size], 'inputs')   \n",
    "            self.action = tf.placeholder(tf.int32, [None], 'action')\n",
    "            # n-step reward and discounted n next step value\n",
    "            self.target = tf.placeholder(tf.float32, [None, 1], 'target')\n",
    "            \n",
    "            self.policy_network = build_policy_network('global_policy')\n",
    "            self.value_network = build_value_network('global_value')\n",
    "            self.policy = self.policy_network(self.inputs, action_size)\n",
    "            self.value = self.value_network(self.inputs)\n",
    "            \n",
    "            self._build_loss_function()\n",
    "            self.update_local, self.update_access = self._build_update()         \n",
    "        \n",
    "    def _build_loss_function(self):\n",
    "        self.advantage = self.target - self.value\n",
    "        # value loss\n",
    "        self.value_loss = tf.reduce_mean(tf.square(self.advantage))\n",
    "    \n",
    "        # policy loss\n",
    "        # get the stochastic policy action probability\n",
    "        #policy_action = _batch_gather(self.policy, self.action)\n",
    "        action_onehot = tf.one_hot(self.action, self.action_size)\n",
    "        policy_action = tf.reduce_sum(self.policy * action_onehot, axis=1, keep_dims=True)\n",
    "        log_policy_action = tf.log(policy_action + _EPSILON)\n",
    "        # no grad pass through advantage in actor network \n",
    "        policy_loss = tf.stop_gradient(self.advantage) * tf.expand_dims(log_policy_action, axis=1)\n",
    "        # entropy loss\n",
    "        entropy_loss = tf.reduce_mean(self.policy * tf.log(self.policy + _EPSILON), axis=1, keep_dims=True)\n",
    "        self.policy_loss = tf.reduce_mean(policy_loss + ENTROPY_BETA * entropy_loss)\n",
    "\n",
    "        # adjust some params\n",
    "        self.a_policy_loss = tf.reduce_mean(policy_loss)\n",
    "        self.a_entropy_loss = tf.reduce_mean(ENTROPY_BETA * entropy_loss)\n",
    "        self.a_value_loss = self.value_loss\n",
    "\n",
    "    def _build_update(self):\n",
    "        global_policy_params, global_value_params = self.Access.get_trainable_variables()\n",
    "        local_policy_params = self.policy_network.get_variables()\n",
    "        local_value_params = self.value_network.get_variables()\n",
    "        \n",
    "        policy_list = []\n",
    "        for g,l in zip(global_policy_params, local_policy_params):\n",
    "            policy_list.append(l.assign(g))\n",
    "            \n",
    "        value_list = []\n",
    "        for g,l in zip(global_value_params, local_value_params):\n",
    "            value_list.append(l.assign(g))        \n",
    "        \n",
    "        policy_grad = tf.gradients(self.policy_loss, list(local_policy_params))\n",
    "        value_grad = tf.gradients(self.value_loss, list(local_value_params))\n",
    "        \n",
    "        policy_apply = self.Access.optimizer_actor.apply_gradients(zip(policy_grad, list(global_policy_params)))\n",
    "        value_apply = self.Access.optimizer_critic.apply_gradients(zip(value_grad, list(global_value_params)))\n",
    "        return [policy_list, value_list], [policy_apply, value_apply]    \n",
    "    \n",
    "    def choose_action(self, SESS, state):  # run by a local\n",
    "        policy = SESS.run(self.policy, {self.inputs: np.expand_dims(state, axis=0)})\n",
    "        policy = np.squeeze(policy)\n",
    "        action = np.random.choice(self.action_space, 1, p=policy)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    def __init__(self, master, name, state_size, action_size):\n",
    "        self.master = master\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.worker = ACNet(self.master, self.state_size, self.action_size, name)\n",
    "    \n",
    "    def work(self, SESS):\n",
    "        worker = self.worker\n",
    "        episode_score_list = []\n",
    "        episode = 0\n",
    "        while episode < MAX_EPISODES:\n",
    "\n",
    "            t_start = t = 1\n",
    "            state = env.reset()\n",
    "\n",
    "            buffer_state = []\n",
    "            buffer_reward = []\n",
    "            buffer_next_state = []\n",
    "            episode_score = 0\n",
    "        \n",
    "            while True:\n",
    "                SESS.run(worker.update_local)\n",
    "                action = worker.choose_action(SESS, state)[0]\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                episode_score += reward\n",
    "\n",
    "                buffer_state.append(state)\n",
    "                buffer_reward.append(reward)\n",
    "                buffer_next_state.append(next_state)\n",
    "                state = next_state\n",
    "\n",
    "                if t - t_start == T_MAX or done:\n",
    "                    t_start = t\n",
    "\n",
    "                    if done:\n",
    "                        state_value = 0\n",
    "                    else:\n",
    "                        state_value = SESS.run(worker.value, {worker.inputs:np.expand_dims(state, axis=0)})[0][0]\n",
    "\n",
    "                    buffer_target = []\n",
    "                    for r in buffer_reward[:-1][::-1]:\n",
    "                        state_value = r + GAMMA * state_value\n",
    "                        buffer_target.append(state_value)\n",
    "                    buffer_target.reverse()\n",
    "\n",
    "                    feed_dict = {worker.inputs: np.vstack(buffer_state[:-1]), \n",
    "                                 worker.action: np.squeeze(np.vstack(buffer_reward[:-1]), axis=1), \n",
    "                                 worker.target: np.expand_dims(np.array(buffer_target), axis=1)}\n",
    "                    SESS.run(worker.update_access, feed_dict)\n",
    "\n",
    "                    if done:\n",
    "                        if episode > 990:\n",
    "                            entropy_loss, policy_loss, value_loss = SESS.run(\n",
    "                                [worker.a_entropy_loss, worker.a_policy_loss, worker.a_value_loss],\n",
    "                                feed_dict)         \n",
    "                            print (entropy_loss, policy_loss, value_loss)\n",
    "\n",
    "                            policy, advantage = SESS.run([worker.policy, worker.advantage], feed_dict)\n",
    "                            print (policy)\n",
    "                            print (advantage)\n",
    "\n",
    "                    buffer_state = [buffer_state[-1]]\n",
    "                    buffer_reward = [buffer_reward[-1]]\n",
    "                    buffer_next_state = [buffer_next_state[-1]]\n",
    "\n",
    "                t += 1\n",
    "                if done:\n",
    "                    episode +=1\n",
    "                    episode_score_list.append(episode_score)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0044441 6.04931 0.322166\n",
      "[[  9.89783347e-01   1.02166114e-02]\n",
      " [  9.99435484e-01   5.64546150e-04]\n",
      " [  9.99963999e-01   3.59785845e-05]\n",
      " [  9.99997258e-01   2.78518382e-06]\n",
      " [  9.99999762e-01   2.61601912e-07]\n",
      " [  1.00000000e+00   2.89356201e-08]\n",
      " [  1.00000000e+00   3.62046060e-09]]\n",
      "[[-0.35994434]\n",
      " [-0.52182007]\n",
      " [-0.56512356]\n",
      " [-0.55521131]\n",
      " [-0.5518806 ]\n",
      " [-0.60306633]\n",
      " [-0.74660766]]\n",
      "-0.00550219 3.20631 0.100407\n",
      "[[  9.86802280e-01   1.31976781e-02]\n",
      " [  9.99240994e-01   7.59073999e-04]\n",
      " [  9.99950051e-01   4.99260932e-05]\n",
      " [  9.99996066e-01   3.97193753e-06]\n",
      " [  9.99999642e-01   3.82888629e-07]\n",
      " [  1.00000000e+00   4.35064145e-08]\n",
      " [  1.00000000e+00   5.60559688e-09]]\n",
      "[[-0.11147928]\n",
      " [-0.27209616]\n",
      " [-0.31276846]\n",
      " [-0.29859519]\n",
      " [-0.29041743]\n",
      " [-0.3378576 ]\n",
      " [-0.48052931]]\n",
      "-0.00387439 -0.832361 0.0118419\n",
      "[[  9.89994347e-01   1.00056324e-02]\n",
      " [  9.99353826e-01   6.46115222e-04]\n",
      " [  9.99952555e-01   4.74875233e-05]\n",
      " [  9.99995828e-01   4.16316516e-06]\n",
      " [  9.99999523e-01   4.35874824e-07]\n",
      " [  1.00000000e+00   5.31836122e-08]\n",
      " [  1.00000000e+00   7.30363414e-09]\n",
      " [  1.00000000e+00   1.08981224e-09]]\n",
      "[[ 0.0733633 ]\n",
      " [-0.02746344]\n",
      " [-0.00756359]\n",
      " [ 0.07208776]\n",
      " [ 0.15443444]\n",
      " [ 0.19270587]\n",
      " [ 0.14933383]\n",
      " [-0.00764465]]\n",
      "-0.0043629 3.45559 0.110763\n",
      "[[  9.90106046e-01   9.89397522e-03]\n",
      " [  9.99398947e-01   6.01056905e-04]\n",
      " [  9.99958038e-01   4.19751013e-05]\n",
      " [  9.99996424e-01   3.53221685e-06]\n",
      " [  9.99999642e-01   3.57734478e-07]\n",
      " [  1.00000000e+00   4.24225419e-08]\n",
      " [  1.00000000e+00   5.67492009e-09]]\n",
      "[[-0.15881586]\n",
      " [-0.30152941]\n",
      " [-0.33143282]\n",
      " [-0.3119328 ]\n",
      " [-0.30155754]\n",
      " [-0.34804308]\n",
      " [-0.48987734]]\n",
      "-0.00438094 -4.53414 0.176606\n",
      "[[  9.88395333e-01   1.16047226e-02]\n",
      " [  9.99245167e-01   7.54837180e-04]\n",
      " [  9.99944329e-01   5.56525156e-05]\n",
      " [  9.99995112e-01   4.89299418e-06]\n",
      " [  9.99999523e-01   5.14363990e-07]\n",
      " [  9.99999881e-01   6.31115782e-08]\n",
      " [  1.00000000e+00   8.72809380e-09]\n",
      " [  1.00000000e+00   1.31330635e-09]]\n",
      "[[ 0.38961649]\n",
      " [ 0.29483128]\n",
      " [ 0.32293177]\n",
      " [ 0.41137862]\n",
      " [ 0.50091767]\n",
      " [ 0.54249763]\n",
      " [ 0.49657881]\n",
      " [ 0.3298347 ]]\n",
      "-0.00410985 3.1667 0.0923076\n",
      "[[  9.90784287e-01   9.21569392e-03]\n",
      " [  9.99446332e-01   5.53708291e-04]\n",
      " [  9.99961615e-01   3.83402439e-05]\n",
      " [  9.99996781e-01   3.20465870e-06]\n",
      " [  9.99999642e-01   3.22685423e-07]\n",
      " [  1.00000000e+00   3.80498122e-08]\n",
      " [  1.00000000e+00   5.05882847e-09]]\n",
      "[[-0.14957476]\n",
      " [-0.28263855]\n",
      " [-0.3043828 ]\n",
      " [-0.27910686]\n",
      " [-0.26549625]\n",
      " [-0.31106389]\n",
      " [-0.45397925]]\n",
      "-0.00553709 2.21548 0.0494995\n",
      "[[  9.86790717e-01   1.32093225e-02]\n",
      " [  9.99190152e-01   8.09851626e-04]\n",
      " [  9.99943495e-01   5.64543589e-05]\n",
      " [  9.99995232e-01   4.72742522e-06]\n",
      " [  9.99999523e-01   4.76931291e-07]\n",
      " [  1.00000000e+00   5.65103093e-08]\n",
      " [  1.00000000e+00   7.58033725e-09]]\n",
      "[[-0.07141733]\n",
      " [-0.20969296]\n",
      " [-0.23094177]\n",
      " [-0.20041251]\n",
      " [-0.17902207]\n",
      " [-0.21667254]\n",
      " [-0.35345364]]\n",
      "-0.00592056 3.22115 0.0976744\n",
      "[[  9.85704124e-01   1.42958490e-02]\n",
      " [  9.99099612e-01   9.00459650e-04]\n",
      " [  9.99935746e-01   6.42054365e-05]\n",
      " [  9.99994516e-01   5.47945183e-06]\n",
      " [  9.99999404e-01   5.62132755e-07]\n",
      " [  9.99999881e-01   6.76897258e-08]\n",
      " [  1.00000000e+00   9.23385635e-09]]\n",
      "[[-0.19345713]\n",
      " [-0.32265186]\n",
      " [-0.33555412]\n",
      " [-0.29708767]\n",
      " [-0.26780891]\n",
      " [-0.29734671]\n",
      " [-0.42567158]]\n",
      "-0.00312463 -9.75356 0.767095\n",
      "[[  9.91260469e-01   8.73959158e-03]\n",
      " [  9.99345720e-01   6.54306728e-04]\n",
      " [  9.99944806e-01   5.51982885e-05]\n",
      " [  9.99994516e-01   5.46536057e-06]\n",
      " [  9.99999404e-01   6.36236734e-07]\n",
      " [  9.99999881e-01   8.52983177e-08]\n",
      " [  1.00000000e+00   1.27751481e-08]\n",
      " [  1.00000000e+00   2.07110018e-09]\n",
      " [  1.00000000e+00   3.53404195e-10]]\n",
      "[[ 0.6590066 ]\n",
      " [ 0.60769796]\n",
      " [ 0.6771903 ]\n",
      " [ 0.81097078]\n",
      " [ 0.95478964]\n",
      " [ 1.06298327]\n",
      " [ 1.09813702]\n",
      " [ 1.02818143]\n",
      " [ 0.82424939]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "GAME = 'CartPole-v0'\n",
    "env = gym.make(GAME)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "MAX_EPISODES = 1000\n",
    "T_MAX = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "SESS = tf.Session()\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    master = Access(state_size, action_size)\n",
    "    worker = Worker(master, 'W0', 4, 2)\n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "    \n",
    "    worker.work(SESS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
