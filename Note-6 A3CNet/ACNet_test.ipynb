{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sonnet.python.modules.basic import Linear\n",
    "from sonnet.python.modules.base import AbstractModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.50000000035e-07\n"
     ]
    }
   ],
   "source": [
    "_EPSILON = 1e-6 # avoid nan\n",
    "\n",
    "def swich(tensor):\n",
    "    return tensor * tf.nn.sigmoid(tensor + _EPSILON)\n",
    "\n",
    "# test for add eps\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "print (sigmoid(_EPSILON) - sigmoid(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared neural network\n",
    "def _build_shared_network(inputs):\n",
    "    # inputs [batch_size, state_size]\n",
    "    network = Linear(32, 'input_layer')(inputs)\n",
    "    network = swich(network)\n",
    "    network = Linear(64, 'hidden_layer')(network)\n",
    "    return swich(network)\n",
    "\n",
    "# build approximate neural network\n",
    "def _build_approximate_network(inputs, action_size):\n",
    "    shared_network = _build_shared_network(inputs)\n",
    "    policy = Linear(action_size, 'policy')(shared_network)\n",
    "    policy = tf.nn.softmax(policy + _EPSILON) # avoid nan   \n",
    "    value = Linear(1, 'value')(shared_network)\n",
    "    return policy, value\n",
    "\n",
    "class simple_approximate_network(AbstractModule):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def _build(self, inputs, action_size):\n",
    "        return _build_approximate_network(inputs, action_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'W1/linear/w:0' shape=(4, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'W1/linear/b:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'W1/linear_1/w:0' shape=(32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'W1/linear_1/b:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'W1/linear_2/w:0' shape=(64, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'W1/linear_2/b:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'W1/linear_3/w:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'W1/linear_3/b:0' shape=(1,) dtype=float32_ref>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "tf.reset_default_graph()\n",
    "\n",
    "W1 = simple_approximate_network('W1')\n",
    "state = tf.placeholder(tf.float32, [5, 4], 'state')\n",
    "W1(state, 2)\n",
    "W1.get_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global network for buffer weights and calculate gardients\n",
    "class Access(object):\n",
    "    def __init__(self, state_size, action_size, name='access'):\n",
    "        #variable_scope for more clear graph, not necessary\n",
    "        with tf.variable_scope(name):                   \n",
    "            # placeholder for state and next state or you may like call it observation\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, state_size], 'inputs')     \n",
    "            self.network = simple_approximate_network('global_network')\n",
    "            self.policy, self.value = self.network(self.inputs, action_size)\n",
    "            \n",
    "        self.trainer = tf.train.RMSPropOptimizer(LEARNING_RATE, name='RMSProp')\n",
    "        \n",
    "    def get_trainable_variables(self):\n",
    "        return self.network.get_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'access/global_network/linear/w:0' shape=(5, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'access/global_network/linear/b:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'access/global_network/linear_1/w:0' shape=(32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'access/global_network/linear_1/b:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'access/global_network/linear_2/w:0' shape=(64, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'access/global_network/linear_2/b:0' shape=(3,) dtype=float32_ref>,\n",
       " <tf.Variable 'access/global_network/linear_3/w:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'access/global_network/linear_3/b:0' shape=(1,) dtype=float32_ref>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "A = Access(5, 3)\n",
    "\n",
    "A.get_trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch gather function from https://github.com/deepmind/dnc/blob/master/util.py\n",
    "def _batch_gather(values, indices):\n",
    "  \"\"\"Returns batched `tf.gather` for every row in the input.\"\"\"\n",
    "  with tf.name_scope('batch_gather', values=[values, indices]):\n",
    "    unpacked = zip(tf.unstack(values), tf.unstack(indices))\n",
    "    result = [tf.gather(value, index) for value, index in unpacked]\n",
    "    return tf.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.29143524 -0.70739245  0.46391261 -0.15822595 -0.08984572  0.17364264]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "tf.reset_default_graph()\n",
    "a = tf.get_variable('values', [6, 3], tf.float32)\n",
    "\n",
    "b = np.random.randint(0, 3, 6)\n",
    "b = tf.convert_to_tensor(b)\n",
    "\n",
    "c = _batch_gather(a, b)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print (sess.run(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENTROPY_BETA = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local network for advantage actor-critic which are also know as A2C\n",
    "class ACNet(object):\n",
    "    def __init__(self, Access, state_size, action_size, name):\n",
    "        self.Access = Access\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # action space, we assume that action space is range(0 to action_size-1)\n",
    "        self.action_space = np.arange(action_size, dtype=np.int32)\n",
    "        \n",
    "        #variable_scope local graph, necessary\n",
    "        with tf.variable_scope(name):\n",
    "            # placeholder for state and next state or you may like call it observation\n",
    "            self.inputs = tf.placeholder(tf.float32, [5, state_size], 'inputs')   \n",
    "            self.action = tf.placeholder(tf.int32, [5], 'action')\n",
    "            # n-step reward and discounted n next step value\n",
    "            self.target = tf.placeholder(tf.float32, [None, 1], 'target')\n",
    "            \n",
    "            self.network = simple_approximate_network('ACNet')\n",
    "            self.policy, self.value = self.network(self.inputs, action_size)\n",
    "            \n",
    "            self._build_loss_function()\n",
    "            self.update_local, self.update_access = self._build_update()\n",
    "            \n",
    "        \n",
    "    def _build_loss_function(self):\n",
    "        self.advantage = self.target - self.value\n",
    "        # value loss\n",
    "        self.value_loss = tf.reduce_mean(tf.square(self.advantage))\n",
    "    \n",
    "        # policy loss\n",
    "        # get the stochastic policy action probability\n",
    "        policy_action = _batch_gather(self.policy, self.action)\n",
    "        log_policy_action = tf.log(policy_action + _EPSILON)\n",
    "        # no grad pass through advantage in actor network \n",
    "        policy_loss = tf.stop_gradient(self.advantage) * tf.expand_dims(log_policy_action, axis=1)\n",
    "        # entropy loss\n",
    "        entropy_loss = tf.reduce_mean(self.policy * tf.log(self.policy + _EPSILON), axis=1, keep_dims=True)\n",
    "        self.policy_loss = tf.reduce_mean(policy_loss + ENTROPY_BETA * entropy_loss)\n",
    "        \n",
    "        self.total_loss = self.value_loss + self.policy_loss\n",
    "        # adjust some params\n",
    "        self.a_policy_loss = tf.reduce_sum(policy_loss)\n",
    "        self.a_entropy_loss = tf.reduce_mean(entropy_loss)\n",
    "        self.a_value_loss = self.value_loss\n",
    "\n",
    "    def _build_update(self):\n",
    "        global_params = list(self.Access.get_trainable_variables())\n",
    "        local_params = list(self.get_trainable_variables())\n",
    "        \n",
    "        # update local network weights\n",
    "        zip_list = []\n",
    "        for g,l in zip(global_params, local_params):\n",
    "            zip_list.append(l.assign(g))\n",
    "        \n",
    "        # update global network gradients\n",
    "        local_grads = tf.gradients(self.total_loss, local_params)\n",
    "        apply_gradients = self.Access.trainer.apply_gradients(zip(local_grads, global_params))\n",
    "        return zip_list, apply_gradients    \n",
    "    \n",
    "    def get_trainable_variables(self):\n",
    "        return self.network.get_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'W1/ACNet/linear/w:0' shape=(5, 32) dtype=float32_ref>, <tf.Variable 'W1/ACNet/linear/b:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'W1/ACNet/linear_1/w:0' shape=(32, 64) dtype=float32_ref>, <tf.Variable 'W1/ACNet/linear_1/b:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'W1/ACNet/linear_2/w:0' shape=(64, 3) dtype=float32_ref>, <tf.Variable 'W1/ACNet/linear_2/b:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'W1/ACNet/linear_3/w:0' shape=(64, 1) dtype=float32_ref>, <tf.Variable 'W1/ACNet/linear_3/b:0' shape=(1,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "A = Access(5, 3)\n",
    "B = ACNet(A, 5, 3, 'W1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
